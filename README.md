# üç∑ Wine Quality Prediction using Machine Learning*
A complete machine learning pipeline for predicting Portuguese redwine quality using classical ML, clustering, dimentioanl reduction, imbalance handling, and model selction.

THis project presents a full data science worklow that performs:
- Exploratory data analysis
- Visualization and correlation inspection
- Clustering using K-Means + PCA
- Handling of class imbalance using SMOTE(Synthetic Minority Over-sampling Technique)
- Training and evaluation of multiple ML classifiers
- Selection of the best-performing model
- Deployment-ready prediction pipeline
# üìä Dataset
- Source Professor Paulo COrtez, University of Minho
- Samples: 1599 wines
- Features: 11 physicochemical attributes
- Target: Quality - converted to binary classification (Good or Bad)

| Class | Meaning              |
| ----- | -------------------- |
| 0     | Bad / Medium quality |
| 1     | Good quality (‚â• 7)   |
* We set 7 as a threshold value to classify Good and Bad, we can set any number to classify
# üß† Pipeline Architecture
``` text 
Raw Dataset
     ‚îÇ
     ‚ñº
Exploratory Data Analysis & Visualization
     ‚îÇ
     ‚ñº
Standard Scaling
     ‚îÇ
     ‚ñº
K-Means Clustering + PCA Visualization
     ‚îÇ
     ‚ñº
Binary Class Conversion (Good / Bad)
     ‚îÇ
     ‚ñº
SMOTE Class Balancing
     ‚îÇ
     ‚ñº
Feature Scaling
     ‚îÇ
     ‚ñº
PCA Dimensionality Reduction (90% variance)
     ‚îÇ
     ‚ñº
Model Training & Evaluation
     ‚îÇ
     ‚ñº
Best Model Selection (Random Forest)
     ‚îÇ
     ‚ñº
Model Saving & New Data Prediction
```
# üìà Models Implemented
| Model               | Accuracy | Precision | Recall | F1-Score |
|--------------------|----------|-----------|--------|----------|
| Logistic Regression| 0.74     | 0.71      | 0.69   | 0.70     |
| K-Nearest Neighbors| 0.81     | 0.79      | 0.77   | 0.78     |
| Support Vector Mach| 0.84     | 0.83      | 0.81   | 0.82     |
| Random Forest      | 0.89     | 0.88      | 0.86   | 0.87     |

                  
   ‚úî Random Forest was selected as the final deployed model
 -** Wine quality is not linear, and its variables are chemically dependent. PCA collapses these nonlinear relations into linear eigenvectors, The pipeline over-compresed because we are working on three distructive operations :**
- SMOTE
- StandardScaler
- PCA (0.90)
  but when we remove the PCA from the Pipeline we got the following : around 91% accuracy on Random Forest 


  

# ‚öñ Handling Imbalanced Data 
The dataset is highly imbalanced.
To avoid misleading accuracy, SMOTE (Synthetic Minority Oversampling Technique) is applied:
| Before SMOTE        | After SMOTE          |
| ------------------- | -------------------- |
| 1382 bad / 217 good | 1105 bad / 1105 good |

  ‚úî This significantly improves precision, recall, and F1 score reliability.

# üßÆ Dimensionality Reduction

PCA retains 90% o the original variance using 7 principal components, improving 
- Training speed
- Generalization
- Noise robustness

  üîç Example Prediction
``` text  new_data = {
 'fixed acidity':7.3,
 'volatile acidity':0.65,
 'citric acid':0.00,
 'residual sugar':1.2,
 'chlorides':0.065,
 'free sulfur dioxide':15,
 'total sulfur dioxide':21,
 'density':0.9946,
 'pH':3.39,
 'sulphates':0.47,
 'alcohol':10
}
```
# Output:
``` text
  Good Quality Wine
```

# üéØ Learning Outcomes

This project demonstrates:
- Data preprocessing & feature engineering
- Visualization & clustering
- Imbalanced data handling
- Model evaluation beyond accuracy
- Real ML deployment workflow











